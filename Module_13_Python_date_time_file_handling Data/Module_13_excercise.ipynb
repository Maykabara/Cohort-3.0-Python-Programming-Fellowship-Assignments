{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 13 Assigment: Level 1\n",
    "## ðŸ’» Python Datetime Exercises\n",
    "### 1. Get the current day, month, year, hour, minute and timestamp from datetime module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-22 20:00:44.634590\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "now = datetime.now()\n",
    "print(now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Format the current date using this format: \"%m/%d/%Y, %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Date: 01/22/2025, 20:00:44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_date = now.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "print(\"Formatted Date:\", formatted_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Today is 5 December, 2019. Change this time string to time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Time: 2019-12-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "time_string = \"5 December, 2019\"\n",
    "converted_time = datetime.strptime(time_string, \"%d %B, %Y\")\n",
    "print(\"Converted Time:\", converted_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calculate the time difference between now and new year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time until New Year: 343 days, 3:59:15.365410\n"
     ]
    }
   ],
   "source": [
    "new_year = datetime(year=now.year + 1, month=1, day=1)\n",
    "time_to_new_year = new_year - now\n",
    "print(\"Time until New Year:\", time_to_new_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Calculate the time difference between 1 January 1970 and now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time since 1 January 1970: 20110 days, 20:00:44.634590\n"
     ]
    }
   ],
   "source": [
    "epoch = datetime(1970, 1, 1)\n",
    "time_since_epoch = now - epoch\n",
    "print(\"Time since 1 January 1970:\", time_since_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Think, what can you use the datetime module for? Examples:\n",
    "- Time series analysis\n",
    "- To get a timestamp of any activities in an application\n",
    "- Adding posts on a blog\n",
    "  ### Examples of Applications:\n",
    "1. **Time Series Analysis**:\n",
    "   - Analyze or manipulate time-based data (e.g., stock prices, weather patterns).\n",
    "   \n",
    "2. **Timestamping Activities**:\n",
    "   - Log activities with timestamps in applications or systems (e.g., logging user actions).\n",
    "\n",
    "3. **Scheduling Tasks or Events**:\n",
    "   - Automate tasks based on specific dates and times (e.g., reminders, alarms).\n",
    "\n",
    "4. **Blog Post Timestamps**:\n",
    "   - Add creation or publication timestamps to blog posts or articles.\n",
    "\n",
    "5. **Performance Tracking**:\n",
    "   - Measure the time taken by code or system processes.\n",
    "\n",
    "6. **Handling Time Zones**:\n",
    "   - Convert times between different time zones (using `pytz` with `datetime`).\n",
    "\n",
    "7. **Calculating Durations**:\n",
    "   - Compute time differences (e.g., between two dates or events).\n",
    "\n",
    "8. **Countdowns and Alarms**:\n",
    "   - Build countdowns for specific events or set alarms.\n",
    "\n",
    "9. **Tracking Deadlines**:\n",
    "   - Manage project deadlines and calculate the time left.\n",
    "\n",
    "10. **Age Calculation**:\n",
    "    - Calculate a personâ€™s age from their birthdate.\n",
    "\n",
    "11. **Validation of Dates**:\n",
    "    - Check whether a date or time input is valid.\n",
    "\n",
    "12. **Data Labeling**:\n",
    "    - Assign timestamps to datasets for record-keeping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’» File Handling Exercises:\n",
    "## Exercises: Level 1\n",
    "### 1. Write a function which count number of lines and number of words in a text. All the files are in the data the folder: a) Read obama_speech.txt file and count number of lines and words b) Read michelle_obama_speech.txt file and count number of lines and words c) Read donald_speech.txt file and count number of lines and words d) Read melina_trump_speech.txt file and count number of lines and words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: obama_speech.txt\n",
      "Number of lines: 66\n",
      "Number of words: 2400\n",
      "\n",
      "File: michelle_obama_speech.txt\n",
      "Number of lines: 83\n",
      "Number of words: 2204\n",
      "\n",
      "File: donald_speech.txt\n",
      "Number of lines: 48\n",
      "Number of words: 1259\n",
      "\n",
      "File: melina_trump_speech.txt\n",
      "Number of lines: 33\n",
      "Number of words: 1375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def count_lines_and_words(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            number_of_lines = len(lines)\n",
    "            number_of_words = sum(len(line.split()) for line in lines)\n",
    "            return number_of_lines, number_of_words\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "        return 0, 0\n",
    "\n",
    "files = [\n",
    "    \"data/obama_speech.txt\",\n",
    "    \"data/michelle_obama_speech.txt\",\n",
    "    \"data/donald_speech.txt\",\n",
    "    \"data/melina_trump_speech.txt\"\n",
    "]\n",
    "\n",
    "for file_path in files:\n",
    "    lines, words = count_lines_and_words(file_path)\n",
    "    print(f\"File: {file_path[5:]}\")\n",
    "    print(f\"Number of lines: {lines}\")\n",
    "    print(f\"Number of words: {words}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the countries_data.json data file in data directory, create a function that finds the ten most spoken languages\n",
    "    # Your output should look like this\n",
    "    print(most_spoken_languages(filename='./data/countries_data.json', 10))\n",
    "    [(91, 'English'),\n",
    "    (45, 'French'),\n",
    "    (25, 'Arabic'),\n",
    "    (24, 'Spanish'),\n",
    "    (9, 'Russian'),\n",
    "    (9, 'Portuguese'),\n",
    "    (8, 'Dutch'),\n",
    "    (7, 'German'),\n",
    "    (5, 'Chinese'),\n",
    "    (4, 'Swahili'),\n",
    "    (4, 'Serbian')]\n",
    "\n",
    "    # Your output should look like this\n",
    "    print(most_spoken_languages(filename='./data/countries_data.json', 3))\n",
    "    [(91, 'English'),\n",
    "    (45, 'French'),\n",
    "    (25, 'Arabic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3179893709.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[21], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(most_spoken_languages(filename='./data/countries_data.json', top_n=3)\u001b[0m\n\u001b[1;37m                                                                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def most_spoken_languages(filename, top_n):\n",
    "    with open(filename, 'r') as file:\n",
    "        countries_data = json.load(file)\n",
    "    languages = []\n",
    "\n",
    "    for country in countries_data:\n",
    "        languages.extend(country.get('languages', []))\n",
    "\n",
    "    language_count = Counter(languages)\n",
    "\n",
    "    most_common_languages = language_count.most_common(top_n)\n",
    "\n",
    "    return [(count, language) for language, count in most_common_languages]\n",
    "\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=10))\n",
    "print(most_spoken_languages(filename='./data/countries_data.json', top_n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the countries_data.json data file in data directory, create a function that creates a list of the ten most populated countries\n",
    "        #  Your output should look like this\n",
    "        print(most_populated_countries(filename='./data/countries_data.json', 10))\n",
    "\n",
    "        [\n",
    "        {'country': 'China', 'population': 1377422166},\n",
    "        {'country': 'India', 'population': 1295210000},\n",
    "        {'country': 'United States of America', 'population': 323947000},\n",
    "        {'country': 'Indonesia', 'population': 258705000},\n",
    "        {'country': 'Brazil', 'population': 206135893},\n",
    "        {'country': 'Pakistan', 'population': 194125062},\n",
    "        {'country': 'Nigeria', 'population': 186988000},\n",
    "        {'country': 'Bangladesh', 'population': 161006790},\n",
    "        {'country': 'Russian Federation', 'population': 146599183},\n",
    "        {'country': 'Japan', 'population': 126960000}\n",
    "        ]\n",
    "\n",
    "        # Your output should look like this\n",
    "\n",
    "        print(most_populated_countries(filename='./data/countries_data.json', 3))\n",
    "        [\n",
    "        {'country': 'China', 'population': 1377422166},\n",
    "        {'country': 'India', 'population': 1295210000},\n",
    "        {'country': 'United States of America', 'population': 323947000}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def most_populated_countries(filename, n):\n",
    "    # Read the JSON file\n",
    "    with open(filename, 'r') as file:\n",
    "        countries_data = json.load(file)\n",
    "    \n",
    "    # Sort countries by population in descending order\n",
    "    sorted_countries = sorted(\n",
    "        countries_data, key=lambda country: country['population'], reverse=True\n",
    "    )\n",
    "    \n",
    "    # Get the top n populated countries\n",
    "    top_countries = sorted_countries[:n]\n",
    "    \n",
    "    # Create the desired output format\n",
    "    result = [{'country': country['name'], 'population': country['population']} for country in top_countries]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "print(most_populated_countries(filename='./data/countries_data.json', n=10))\n",
    "print(most_populated_countries(filename='./data/countries_data.json', n=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises: Level 2\n",
    "### 4. Extract all incoming email addresses as a list from the email_exchange_big.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_email_addresses(file_path):\n",
    "    # Define a regex pattern for email addresses\n",
    "    email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
    "    \n",
    "    # Initialize an empty list to store email addresses\n",
    "    email_addresses = []\n",
    "    \n",
    "    # Open and read the file\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                # Find all email addresses in the current line\n",
    "                matches = re.findall(email_pattern, line)\n",
    "                email_addresses.extend(matches)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {file_path} was not found.\")\n",
    "    \n",
    "    # Remove duplicates by converting to a set and back to a list\n",
    "    return list(set(email_addresses))\n",
    "\n",
    "\n",
    "# File path to the email_exchange_big.txt\n",
    "file_path = \"email_exchange_big.txt\"\n",
    "\n",
    "# Extract email addresses\n",
    "emails = extract_email_addresses(file_path)\n",
    "\n",
    "# Print the extracted email addresses\n",
    "print(f\"Extracted {len(emails)} email addresses:\")\n",
    "print(emails)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Find the most common words in the English language. Call the name of your function find_most_common_words, it will take two parameters - a string or a file and a positive integer, indicating the number of words. Your function will return an array of tuples in descending order. Check the output\n",
    "        # Your output should look like this\n",
    "            print(find_most_common_words('sample.txt', 10))\n",
    "            [(10, 'the'),\n",
    "            (8, 'be'),\n",
    "            (6, 'to'),\n",
    "            (6, 'of'),\n",
    "            (5, 'and'),\n",
    "            (4, 'a'),\n",
    "            (4, 'in'),\n",
    "            (3, 'that'),\n",
    "            (2, 'have'),\n",
    "            (2, 'I')]\n",
    "\n",
    "            # Your output should look like this\n",
    "            print(find_most_common_words('sample.txt', 5))\n",
    "\n",
    "            [(10, 'the'),\n",
    "            (8, 'be'),\n",
    "            (6, 'to'),\n",
    "            (6, 'of'),\n",
    "            (5, 'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(find_most_common_words('sample.txt', 5)) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use the function, find_most_frequent_words to find: a) The ten most frequent words used in Obama's speech b) The ten most frequent words used in Michelle's speech c) The ten most frequent words used in Trump's speech d) The ten most frequent words used in Melina's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(f\"The ten most frequent words used in Obama's speech: {find_most_common_words('data/obama_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Michelle's speech: {find_most_common_words('data/michelle_obama_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Melina's speech: {find_most_common_words('data/melina_trump_speech.txt', 10)}\")\n",
    "print(f\"The ten most frequent words used in Trump's speech: {find_most_common_words('data/donald_speech.txt', 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Write a python application that checks similarity between two texts. It takes a file or a string as a parameter and it will evaluate the similarity of the two texts. For instance check the similarity between the transcripts of Michelle's and Melina's speech. You may need a couple of functions, function to clean the text(clean_text), function to remove support words(remove_support_words) and finally to check the similarity(check_text_similarity). List of stop words are in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from Data import stop_words  # Import stop words from the stop_words.py file\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the input text by removing punctuation and converting to lowercase.\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def remove_support_words(text):\n",
    "    \"\"\"Remove stop words from the text using the imported stop_words list.\"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return filtered_words\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    \"\"\"Calculate the Jaccard similarity between two lists.\"\"\"\n",
    "    intersection = len(set(list1) & set(list2))\n",
    "    union = len(set(list1) | set(list2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def process_text(file1, file2):\n",
    "    \"\"\"Process the input text files, clean them, and remove stop words.\"\"\"\n",
    "    # Read the text files\n",
    "    with open(file1, 'r') as f:\n",
    "        text1 = f.read()\n",
    "    with open(file2, 'r') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    # Clean the text and remove stop words, returning word lists\n",
    "    text1 = remove_support_words(clean_text(text1))\n",
    "    text2 = remove_support_words(clean_text(text2))\n",
    "\n",
    "    # Calculate Jaccard similarity using word lists\n",
    "    return jaccard_similarity(text1, text2)\n",
    "\n",
    "# Example usage:\n",
    "similarity = process_text(file1='data/melina_trump_speech.txt', file2='data/michelle_obama_speech.txt')\n",
    "print(f\"Jaccard similarity score between the two speeches: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Find the 10 most repeated words in the romeo_and_juliet.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(source, n):\n",
    "    if isinstance(source, str):\n",
    "        try:\n",
    "            with open(source, 'r') as file:\n",
    "                text = file.read()\n",
    "        except FileNotFoundError:\n",
    "            text = source  # If file not found, treat source as a string\n",
    "    else:\n",
    "        raise ValueError(\"Source must be a filename (string) or text content.\")\n",
    "\n",
    "    # Normalize text by removing punctuation and converting to lowercase\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    \n",
    "    # Count word frequencies\n",
    "    word_count = Counter(words)\n",
    "    \n",
    "    # Get the n most common words\n",
    "    most_common = word_count.most_common(n)\n",
    "    \n",
    "    return [(count, word) for word, count in most_common]\n",
    "\n",
    "print(f\"The ten most repeated word in romeo_and_juliet.txt: {find_most_common_words('data/romeo_and_juliet.txt', 10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Read the hacker news csv file and find out: a) Count the number of lines containing python or Python b) Count the number lines containing JavaScript, javascript or Javascript c) Count the number lines containing Java and not JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def count_keywords_in_csv(file_path):\n",
    "    # Initialize counters\n",
    "    count_python = 0\n",
    "    count_javascript = 0\n",
    "    count_java_not_javascript = 0\n",
    "\n",
    "    # Open the CSV file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        \n",
    "        # Assuming the title/content is in the first column (index 0)\n",
    "        for row in reader:\n",
    "            if len(row) > 0:  # Check if the row has content\n",
    "                text = row[0].lower()  # Assuming the content to check is in the first column\n",
    "                # Check for Python-related occurrences\n",
    "                if 'python' in text:\n",
    "                    count_python += 1\n",
    "                # Check for JavaScript-related occurrences\n",
    "                if 'javascript' in text:\n",
    "                    count_javascript += 1\n",
    "                # Check for Java but not JavaScript\n",
    "                if 'java' in text and 'javascript' not in text:\n",
    "                    count_java_not_javascript += 1\n",
    "\n",
    "    return count_python, count_javascript, count_java_not_javascript\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'data/hacker_news.csv'  # Ensure this is the correct file path\n",
    "python_count, javascript_count, java_not_js_count = count_keywords_in_csv(file_path)\n",
    "\n",
    "print(f\"Count of lines containing 'python' or 'Python': {python_count}\")\n",
    "print(f\"Count of lines containing 'JavaScript', 'javascript', or 'Javascript': {javascript_count}\")\n",
    "print(f\"Count of lines containing 'Java' but not 'JavaScript': {java_not_js_count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myDS-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
